{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA5: Estimating Classifier Accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Programmer: Lydia Lonzarich\n",
    "- Class: CPSC 322-01, Fall 2025\n",
    "- Programming Assignment #5\n",
    "- Date of current version: 11/6/2025\n",
    "- Description: this notebook evaluates the classification accuracy of kNN and dummy classifiers using different methods of dataset splitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "from mysklearn.myutils import random_subsample, cross_val_predict, bootstrap_method\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation\n",
    "from mysklearn.myevaluation import confusion_matrix\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"auto-data-removed-NA.txt\"\n",
    "pytable = MyPyTable()\n",
    "pytable.load_from_file(filename)\n",
    "\n",
    "# convert all integer values to floats.\n",
    "pytable.convert_to_numeric()\n",
    "\n",
    "# convert the dataset to an array.\n",
    "data = np.array(pytable.data, dtype=object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Train / Test Sets: Random Sub-sampling\n",
    "- In this step, I evaluate the performance of both the kNN and dummy classifiers on predicting DOE mpg ratings using random sub-sampling sampling strategy. I used the 'cylinders', 'weight', and 'acceleration' attributes and a randomized 2:1 train-test split ratio of the dataset, repeated k=10 times, to generate these predictions.\n",
    "- Note: random subsampling == repeated random train/test splits, and is used to get a robust estimate of performance\n",
    "- I assume this method for evaluating classifier accuracy will be least effective because we don't guarantee that all instances will appear in the train and test set at least once, nor that each instance will appear in the train/test set equally as often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "STEP 1: Predictive Accuracy\n",
      "=============================\n",
      "Random Subsample (k=10, 2:1 Train/Test)\n",
      "k Nearest Neighbors Classifier: accuracy =  0.37209302325581395 , error rate =  0.627906976744186\n",
      "Dummy Classifier: accuracy =  0.09069767441860466 , error rate =  0.9093023255813953\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================\")\n",
    "print(\"STEP 1: Predictive Accuracy\")\n",
    "print(\"=============================\")\n",
    "print(\"Random Subsample (k=10, 2:1 Train/Test)\")\n",
    "\n",
    "# ** \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of the 'cylinders', 'weight', 'acceleration', and 'mpg' column in the table.\n",
    "cylinder_indices = pytable.column_names.index(\"cylinders\")\n",
    "weight_indices = pytable.column_names.index(\"weight\")\n",
    "acceleration_indices = pytable.column_names.index(\"acceleration\")\n",
    "mpg_indices = pytable.column_names.index(\"mpg\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = np.column_stack((data[:, cylinder_indices], data[:, weight_indices], data[:, acceleration_indices]))\n",
    "y = data[:, mpg_indices]\n",
    "\n",
    "# normalize features in X_train and X_test using min-max normalization.\n",
    "min = X.min(axis=0)\n",
    "max = X.max(axis=0)\n",
    "X = (X - min) / (max - min)\n",
    "\n",
    "# compute the avg acc and error rate for each train/test split of the data.\n",
    "knn_acc, knn_err_rate = random_subsample(X, y, 10, lambda: MyKNeighborsClassifier(n_neighbors=5)) # lambda creates a fresh instance with parameters when I call classifier_class() in the random_subsample function in utils.py. \n",
    "dummy_acc, dummy_err_rate = random_subsample(X, y, 10, MyDummyClassifier)\n",
    "\n",
    "print(\"k Nearest Neighbors Classifier: accuracy = \", knn_acc, \", error rate = \", knn_err_rate)\n",
    "print(\"Dummy Classifier: accuracy = \", dummy_acc, \", error rate = \", dummy_err_rate)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train / Test Sets: Cross Validation\n",
    "- In this step, I compute the predictive acccuracy for both the kNN and dummy classifiers using k-fold cross validation with k=10.\n",
    "- Note: I assume cross validation is a very reliable method estimating classifier accuracy becuase it splits the data and trains the model only on unseen instances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified cross validation commentary: \n",
    "- The stratified cross validation did not significantly improve the kNN classifier performance. Though it did slightly improve the dummy classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "STEP 2: Predictive Accuracy\n",
      "=============================\n",
      "10-Fold Cross Validation\n",
      "k Nearest Neighbors Classifier: accuracy =  0.35 , error rate =  0.65\n",
      "Dummy Classifier: accuracy =  0.05 , error rate =  0.95\n",
      "k Nearest Neighbors Classifier with stratified cross validation: accuracy =  0.31109499637418414 , error rate =  0.6889050036258159\n",
      "Dummy Classifier with stratified cross validation: accuracy =  0.13517041334300217 , error rate =  0.8648295866569979\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================\")\n",
    "print(\"STEP 2: Predictive Accuracy\")\n",
    "print(\"=============================\")\n",
    "print(\"10-Fold Cross Validation\")\n",
    "\n",
    "# ** \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of the 'cylinders', 'weight', 'acceleration', and 'mpg' column in the table.\n",
    "cylinder_indices = pytable.column_names.index(\"cylinders\")\n",
    "weight_indices = pytable.column_names.index(\"weight\")\n",
    "acceleration_indices = pytable.column_names.index(\"acceleration\")\n",
    "mpg_indices = pytable.column_names.index(\"mpg\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = np.column_stack((data[:, cylinder_indices], data[:, weight_indices], data[:, acceleration_indices]))\n",
    "y = data[:, mpg_indices]\n",
    "\n",
    "# normalize features in X_train and X_test using min-max normalization.\n",
    "min = X.min(axis=0)\n",
    "max = X.max(axis=0)\n",
    "X = (X - min) / (max - min)\n",
    "\n",
    "# compute the avg acc and error rate for each train/test split of the data.\n",
    "knn_acc, knn_err_rate, knn_y_trues, knn_y_preds = cross_val_predict(X, y, 10, lambda: MyKNeighborsClassifier(n_neighbors=5), False)\n",
    "dummy_acc, dummy_err_rate, dummy_y_trues, dummy_y_preds = cross_val_predict(X, y, 10, MyDummyClassifier, False)\n",
    "\n",
    "print(\"k Nearest Neighbors Classifier: accuracy = \", knn_acc, \", error rate = \", knn_err_rate)\n",
    "print(\"Dummy Classifier: accuracy = \", dummy_acc, \", error rate = \", dummy_err_rate)\n",
    "\n",
    "\n",
    "# compute the avg acc and error rate for each train/test split of the data using stratified cross validation.\n",
    "knn_acc2, knn_err_rate2, knn_y_trues2, knn_y_preds2 = cross_val_predict(X, y, 10, lambda: MyKNeighborsClassifier(n_neighbors=5), True)\n",
    "dummy_acc2, dummy_err_rate2, dummy_y_trues2, dummy_y_preds2 = cross_val_predict(X, y, 10, MyDummyClassifier, True)\n",
    "\n",
    "print(\"k Nearest Neighbors Classifier with stratified cross validation: accuracy = \", knn_acc2, \", error rate = \", knn_err_rate2)\n",
    "print(\"Dummy Classifier with stratified cross validation: accuracy = \", dummy_acc2, \", error rate = \", dummy_err_rate2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train / Test Sets: Bootstrap Method\n",
    "- In this step, I compute the predictive accuracy and error rate for each classifier using the bootstrap method with k=10. \n",
    "- Note: I assume this method of creating train/test splits and evaluating the model on each split might give higher accuracy and lower error rate than the two previous methods because the training and testing data overlap == seen instances could be \"reused\" for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "STEP 3: Predictive Accuracy\n",
      "=============================\n",
      "k=10 Bootstrap Method\n",
      "k Nearest Neighbors Classifier: accuracy =  0.3320560354375474 , error rate =  0.6679439645624525\n",
      "Dummy Classifier: accuracy =  0.10852384531400827 , error rate =  0.8914761546859917\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================\")\n",
    "print(\"STEP 3: Predictive Accuracy\")\n",
    "print(\"=============================\")\n",
    "print(\"k=10 Bootstrap Method\")\n",
    "\n",
    "# ** \"get\" X (data) and y (corresponding labels) out of the pytable.\n",
    "# find indices of the 'cylinders', 'weight', 'acceleration', and 'mpg' column in the table.\n",
    "cylinder_indices = pytable.column_names.index(\"cylinders\")\n",
    "weight_indices = pytable.column_names.index(\"weight\")\n",
    "acceleration_indices = pytable.column_names.index(\"acceleration\")\n",
    "mpg_indices = pytable.column_names.index(\"mpg\")\n",
    "\n",
    "# separate data into X (samples) and y (corresponding labels).\n",
    "X = np.column_stack((data[:, cylinder_indices], data[:, weight_indices], data[:, acceleration_indices]))\n",
    "y = data[:, mpg_indices]\n",
    "\n",
    "# normalize features in X_train and X_test using min-max normalization.\n",
    "min = X.min(axis=0)\n",
    "max = X.max(axis=0)\n",
    "X = (X - min) / (max - min)\n",
    "\n",
    "# compute the avg acc and error rate for each train/test split of the data.\n",
    "knn_acc, knn_err_rate = bootstrap_method(X, y, 10, lambda: MyKNeighborsClassifier(n_neighbors=5))\n",
    "dummy_acc, dummy_err_rate = bootstrap_method(X, y, 10, MyDummyClassifier)\n",
    "\n",
    "print(\"k Nearest Neighbors Classifier: accuracy = \", knn_acc, \", error rate = \", knn_err_rate)\n",
    "print(\"Dummy Classifier: accuracy = \", dummy_acc, \", error rate = \", dummy_err_rate)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Confusion Matrices\n",
    "- In this step, I create a confusion matrix for each classifier based on the 10-fold stratified cross validation results.\n",
    "- I used the tabulate package to display a pretty confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "STEP 4: Confusion Matrices\n",
      "=============================\n",
      "kNN Classifier (Stratified 10-Fold Cross Validation Results):\n",
      "-------------------------------------------------------------------\n",
      "Dummy Classifier (Stratified 10-Fold Cross Validation Results):\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|   MPG Ranking |   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |   10 |   Total |   Recognition (%) |\n",
      "+===============+=====+=====+=====+=====+=====+=====+=====+=====+=====+======+=========+===================+\n",
      "|             1 |   7 |  10 |  11 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      28 |              25   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             2 |   7 |   1 |   8 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      16 |               6.2 |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             3 |  10 |  16 |   5 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      31 |              16.1 |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             4 |  24 |  11 |  15 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      50 |               0   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             5 |  25 |  15 |  11 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      51 |               0   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             6 |  12 |   9 |  14 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      35 |               0   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             7 |  10 |   9 |   7 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      26 |               0   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             8 |   8 |   6 |   6 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |      20 |               0   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|             9 |   1 |   1 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |       3 |               0   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n",
      "|            10 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |    0 |       0 |               0   |\n",
      "+---------------+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+---------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================\")\n",
    "print(\"STEP 4: Confusion Matrices\")\n",
    "print(\"=============================\")\n",
    "\n",
    "labels = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "\n",
    "headers = [\"MPG Ranking\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Total\", \"Recognition (%)\"]\n",
    "\n",
    "\n",
    "\n",
    "print(\"kNN Classifier (Stratified 10-Fold Cross Validation Results):\")\n",
    "knn_matrix = confusion_matrix(knn_y_trues2, knn_y_preds2, labels) # ==> a list of lists.\n",
    "knn_matrix = np.array(knn_matrix)\n",
    "totals = knn_matrix.sum(axis=1) # get the totals for each class in the table.\n",
    "\n",
    "# initialize a list to store the recognition % for each row. \n",
    "knn_recognition = [] \n",
    "\n",
    "# iterate through each row in the knn_matrix to calculate its recognition %.\n",
    "# recognition: - another measure of how well the model predicted the true label. \n",
    "#              - we look at how close the diagonal values in the cm (row i, column i) is to the the total for a row i. \n",
    "for row in range(len(knn_matrix)):\n",
    "    # if there are instances the current row, calcuate its recognition %: diagonal / row_total * 100 \n",
    "    if totals[row] > 0:\n",
    "        rec = knn_matrix[row, row] / totals[row] * 100\n",
    "    else:\n",
    "        rec = 0\n",
    "\n",
    "    knn_recognition.append(rec)\n",
    "\n",
    "# add the total counts and recognition (%) of each row to cm matrix. \n",
    "completed_knn_cm = []\n",
    "for i, label in enumerate(labels):\n",
    "    # for each row, append the data as: mpg ranking label | the actual data for each ranking | total count for the row | recognition percept for the row\n",
    "    completed_knn_cm.append([label] + list(knn_matrix[i]) + [totals[i], round(knn_recognition[i], 1)])\n",
    "\n",
    "# create the formatted cm. \n",
    "knn_cm_table = tabulate(completed_knn_cm, headers=headers, tablefmt=\"grid\")\n",
    "print(knn_cm_table)\n",
    "\n",
    "\n",
    "\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"Dummy Classifier (Stratified 10-Fold Cross Validation Results):\")\n",
    "dummy_matrix = confusion_matrix(dummy_y_trues, dummy_y_preds, labels)\n",
    "dummy_matrix = np.array(dummy_matrix)\n",
    "totals = dummy_matrix.sum(axis=1) # get the totals for each class in the table.\n",
    "\n",
    "# initialize a list to store the recognition % for each row. \n",
    "dummy_recognition = [] \n",
    "\n",
    "# iterate through each row in the knn_matrix to calculate its recognition %.\n",
    "for row in range(len(dummy_matrix)):\n",
    "    # if there are instances the current row, calcuate its recognition %: diagonal / row_total * 100 \n",
    "    if totals[row] > 0:\n",
    "        rec = dummy_matrix[row, row] / totals[row] * 100\n",
    "    else:\n",
    "        rec = 0\n",
    "\n",
    "    dummy_recognition.append(rec)\n",
    "\n",
    "# add the total counts and recognition (%) of each row to cm matrix. \n",
    "completed_dummy_cm = []\n",
    "for i, label in enumerate(labels):\n",
    "    # for each row, append the data as: mpg ranking label | the actual data for each ranking | total count for the row | recognition percept for the row\n",
    "    completed_dummy_cm.append([label] + list(dummy_matrix[i]) + [totals[i], round(dummy_recognition[i], 1)])\n",
    "\n",
    "# create the formatted cm. \n",
    "dummy_cm_table = tabulate(completed_dummy_cm, headers=headers, tablefmt=\"grid\")\n",
    "print(dummy_cm_table)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Reflection\n",
    "- The low accuracy of my classifiers in steps 1, 2, and 3 could be due to the fact that our prediction class (DOE MPG ratings) has 10 possible values (1-10). The model can struggle to predict this many class labels with only three features, especially given a model our models are very simple. \n",
    "- I did refer to AI in some areas of my PA. However, in all such cases, I did not copy/paste code, nor did rely on it for understanding. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4 (v3.12.4:8e8a4baf65, Jun  6 2024, 17:33:18) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
